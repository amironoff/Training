{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Corpus revision=4427619 docs:4022 tokens:84046>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import opencorpora\n",
    "import string\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "\n",
    "print(sklearn.__version__)\n",
    "corpus = opencorpora.load(\"data/annot.opcorpora.no_ambig.xml\")\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_sentences():\n",
    "    return_value = []\n",
    "    for sentence in corpus.sentences:\n",
    "        sample = []\n",
    "        for token in sentence.tokens:\n",
    "            token_text = normalize_token(token.source)\n",
    "            if token_text:\n",
    "                sample.append([token_text, \n",
    "                               token.lemma,\n",
    "                               token.parses[0].grammemes[0]])\n",
    "        if len(sample) > 0:\n",
    "            return_value.append(sample)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "\n",
    "def get_sentences_from_text(text):\n",
    "    return_value = []\n",
    "    # poor man's sentence breaking\n",
    "    for sentence in text.split('.'):\n",
    "        sample = []\n",
    "        # poor man's tokenizer\n",
    "        for token in sentence.split(' '):\n",
    "            token = normalize_token(token)\n",
    "            if token:\n",
    "                sample.append([token])\n",
    "        if len(sample) > 0:        \n",
    "            return_value.append(sample)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "\n",
    "def normalize_token(token: str, fix_diacritics=False):\n",
    "    return_value = None\n",
    "    punctuation = set(['»', '«', '–', ',']).union(string.punctuation)\n",
    "    if token and token not in punctuation:\n",
    "        return_value = ''.join([c for c in token if c not in punctuation])\n",
    "        if fix_diacritics:\n",
    "            return_value = return_value.replace('ё', 'e')\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['стала'],\n  ['стабильнее'],\n  ['экономическая'],\n  ['и'],\n  ['политическая'],\n  ['обстановка']]]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"стала стабильнее экономическая; и политическая обстановка, \"\n",
    "get_sentences_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = get_training_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import codecs\n",
    "f = codecs.open(\"data/odict.csv\", 'r', 'cp1251')\n",
    "\n",
    "lemmas = {}\n",
    "for line in f:\n",
    "        columns = line.split(\",\")\n",
    "        # the first two columns are\n",
    "        # lemma and tense\n",
    "        lemma = normalize_token(columns[0].lower(), fix_diacritics=True)\n",
    "        if len(columns) > 2:\n",
    "            for word in columns[2:]:\n",
    "                word = normalize_token(word, fix_diacritics=True)\n",
    "                lemmas[word] = lemma    \n",
    "        lemmas[lemma] = lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas['всем'] = 'все'\n",
    "lemmas['всех'] = 'все'\n",
    "lemmas['все'] = 'все'\n",
    "lemmas['гришины'] = 'гриша'\n",
    "lemmas['был'] = 'быть'\n",
    "lemmas['кого'] = 'кто'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'все'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[\"всем\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "    ]\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word[-3:]=' + word1[-3:],\n",
    "            '-1:word[-2:]=' + word1[-2:],\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word[-3:]=' + word1[-3:],\n",
    "            '+1:word[-2:]=' + word1[-2:],\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "                \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [postag for token, lemma, postag in sent]\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, lemma, postag in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Сохранится', 'сохранился', 'VERB'],\n  ['ли', 'ли', 'PRCL'],\n  ['градус', 'градус', 'NOUN'],\n  ['дискуссии', 'дискуссия', 'NOUN'],\n  ['в', 'в', 'PREP'],\n  ['новом', 'новый', 'ADJF'],\n  ['сезоне', 'сезон', 'NOUN']],\n [['Великолепная', 'великолепный', 'ADJF'],\n  ['Школа', 'школа', 'NOUN'],\n  ['злословия', 'злословие', 'NOUN'],\n  ['вернулась', 'вернулся', 'VERB'],\n  ['в', 'в', 'PREP'],\n  ['эфир', 'эфир', 'NOUN'],\n  ['после', 'после', 'PREP'],\n  ['летних', 'летний', 'ADJF'],\n  ['каникул', 'каникулы', 'NOUN'],\n  ['в', 'в', 'PREP'],\n  ['новом', 'новый', 'ADJF'],\n  ['формате', 'формат', 'NOUN']],\n [['В', 'в', 'PREP'],\n  ['истории', 'история', 'NOUN'],\n  ['программы', 'программа', 'NOUN'],\n  ['это', 'это', 'NPRO'],\n  ['уже', 'уже', 'ADVB'],\n  ['не', 'не', 'PRCL'],\n  ['первый', 'первый', 'ADJF'],\n  ['ребрендинг', 'ребрендинг', 'UNKN']],\n [['Писательница', 'писательница', 'NOUN'],\n  ['Татьяна', 'татьяна', 'NOUN'],\n  ['Толстая', 'толстая', 'NOUN'],\n  ['и', 'и', 'CONJ'],\n  ['сценаристка', 'сценаристка', 'NOUN'],\n  ['Дуня', 'дуня', 'NOUN'],\n  ['Смирнова', 'смирнова', 'NOUN'],\n  ['вроде', 'вроде', 'PRCL'],\n  ['бы', 'бы', 'PRCL'],\n  ['не', 'не', 'PRCL'],\n  ['вполне', 'вполне', 'ADVB'],\n  ['соответствовали', 'соответствую', 'VERB'],\n  ['принятым', 'принятый', 'ADJF'],\n  ['на', 'на', 'PREP'],\n  ['российском', 'российский', 'ADJF'],\n  ['телевидении', 'телевидение', 'NOUN'],\n  ['стандартам', 'стандарт', 'NOUN'],\n  ['телеведущих', 'телеведущая', 'NOUN']]]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sentences[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 506 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [sent2features(s) for s in training_sentences]\n",
    "y_train = [sent2labels(s) for s in training_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('data/opencorpora-pos.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x203bd3cdda0>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('data/opencorpora-pos.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Школа злословия учит прикусить язык\n\nPredicted: NOUN NOUN VERB INFN NOUN\nCorrect:   NOUN NOUN VERB INFN NOUN\n"
     ]
    }
   ],
   "source": [
    "example_sent = training_sentences[0]\n",
    "print(' '.join(sent2tokens(example_sent)), end='\\n\\n')\n",
    "\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))\n",
    "print(\"Correct:  \", ' '.join(sent2labels(example_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tag_to_task(tag: str):\n",
    "    #return tag\n",
    "    if tag.startswith(\"NOUN\"):\n",
    "        return \"S\"\n",
    "    if tag.startswith(\"NPR\"):\n",
    "        return \"NI\"\n",
    "    if tag.startswith(\"V\"):\n",
    "        return \"V\"\n",
    "    if tag.startswith(\"INFN\"):\n",
    "        return \"V\"\n",
    "    if tag.startswith(\"PRT\"):\n",
    "        return \"V\"\n",
    "    if tag.startswith(\"ADJ\"):\n",
    "        return \"A\"\n",
    "    if tag.startswith(\"COMP\"):\n",
    "        return \"ADV\"\n",
    "    if tag.startswith(\"PRCL\"):\n",
    "        return \"ADV\"\n",
    "    if tag.startswith(\"PREP\"):\n",
    "        return \"PR\"\n",
    "    if tag.startswith(\"CONJ\"):\n",
    "        return \"CONJ\"\n",
    "    return \"ADV\"\n",
    "        \n",
    "\n",
    "def word2pos(sent, i, tags):\n",
    "    word = sent[i][0]\n",
    "    tag = tags[i]\n",
    "    if word:\n",
    "        try:\n",
    "            lemma = normalize_token(word.lower(), fix_diacritics=True)\n",
    "            if lemma in lemmas:\n",
    "                lemma = lemmas[lemma]\n",
    "            return word+\"{\"+lemma+\"=\"+map_tag_to_task(tag.strip()) + \"}\"\n",
    "        except:\n",
    "            print(word + \":\" + sent)\n",
    "\n",
    "\n",
    "def text2pos(sent, tagger):\n",
    "    tags = tagger.tag(sent2features(sent))\n",
    "    return ' '.join([word2pos(sent, i, tags) for i in range(len(sent))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas['всем'] = 'все'\n",
    "lemmas['всех'] = 'все'\n",
    "lemmas['все'] = 'все'\n",
    "lemmas['гришины'] = 'гриша'\n",
    "lemmas['был'] = 'быть'\n",
    "lemmas['кого'] = 'кто'\n",
    "lemmas['пахры'] = 'пахра'\n",
    "lemmas['этому'] = 'этот'\n",
    "lemmas['людей'] = 'человек'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'А{а=CONJ} все{все=S} дёдовы{дeдовы=S} авантюры{авантюра=S} только{только=ADV} ещё{ещe=S} более{более=ADV} подтачивали{подтачивать=V} его{его=A} здоровье{здоровье=S}'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \\\n",
    "    get_sentences_from_text(\"Стала стабильнее экономическая и политическая обстановка, \"\n",
    "                            \"предприятия вывели из тени зарплаты сотрудников.\"\n",
    "                            \"Все Гришины одноклассники уже побывали за границей, \"\n",
    "                            \"он был чуть ли не единственным, кого не вывозили никуда\"\n",
    "                            \" дальше Красной Пахры.\"\n",
    "                            \"А все дёдовы авантюры только ещё более подтачивали его здоровье.\")\n",
    "text2pos(sentences[2], tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.txt', mode='w', encoding='utf8') as output:\n",
    "    with open('data\\dataset_37845_1.txt', encoding='utf8') as input:\n",
    "        for line in input:\n",
    "            line = line.strip()\n",
    "            sentences = get_sentences_from_text(line)\n",
    "            for sentence in sentences:\n",
    "                output.write(text2pos(sentence, tagger) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}